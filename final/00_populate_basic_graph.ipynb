{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drewdahlquist/CS8750/blob/main/final/00_populate_basic_graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's get started with Method 1...\n",
        "\n",
        "If you are running this notebook on Google Colab, run the following cell:"
      ],
      "metadata": {
        "id": "VhUT4K1c_L6j"
      },
      "id": "VhUT4K1c_L6j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install py2neo\n",
        "!pip install wikipedia\n",
        "!pip install spacy # ==3.0.3"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting py2neo\n",
            "  Downloading py2neo-2021.2.3-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.0/177.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from py2neo) (1.16.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from py2neo) (23.1)\n",
            "Collecting monotonic\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: pygments>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from py2neo) (2.14.0)\n",
            "Collecting pansi>=2020.7.3\n",
            "  Downloading pansi-2020.7.3-py2.py3-none-any.whl (10 kB)\n",
            "Collecting interchange~=2021.0.4\n",
            "  Downloading interchange-2021.0.4-py2.py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from py2neo) (1.26.15)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from py2neo) (2022.12.7)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from interchange~=2021.0.4->py2neo) (2022.7.1)\n",
            "Installing collected packages: monotonic, pansi, interchange, py2neo\n",
            "Successfully installed interchange-2021.0.4 monotonic-1.6 pansi-2020.7.3 py2neo-2021.2.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.4.1)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11696 sha256=c7e1da1bfc88870d749be15d4dd5b8ef1a6fb6fa13a1e026e5f4719ec03792a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n"
          ]
        }
      ],
      "metadata": {
        "id": "S2dRYrKj_L6l",
        "outputId": "b290134a-65f4-4f9a-8ad8-123e2b64dcba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "S2dRYrKj_L6l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import json\n",
        "import re\n",
        "import urllib\n",
        "from pprint import pprint\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from py2neo import Node, Graph, Relationship, NodeMatcher\n",
        "from py2neo.bulk import merge_nodes\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wikipedia\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.tokens import Doc, Span, Token\n",
        "\n",
        "print(spacy.__version__)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.2\n"
          ]
        }
      ],
      "metadata": {
        "id": "kFuKUib5_L6n",
        "outputId": "27267dab-22d5-4fd1-f8c6-b5452a0630fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kFuKUib5_L6n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure spacy\n",
        "\n",
        "Prior to actually using spacy, we need to load in some models.  The basic model is their small core library, taken from the web: `en_core_web_sm`, which provides good, basic functionality with a small download size (< 20 MB).  However, one drawback of this basic model is that it doesn't have full word vectors.  Instead, it comes with context-sensitive tensors.  You can still do things like text similarity with it, but if you want to use spacy to create good word vectors, you should use a larger model such as `en_core_web_md` or`en_core_web_lg` since the small models are not known for accuracy.  You can also use a variety of third-party models, but that is beyond the scope of this workshop.  Again, choose the model that works best with your setup.\n",
        "\n",
        "In general, to load the models we use the following command:\n",
        "\n",
        "`python3 -m spacy download en_core_web_md`\n",
        "\n",
        "This has already been completed in the container, but if you want to add other models you can do this either as a cell in this notebook or via the CLI.\n",
        "\n",
        "## API key for Google Knowledge Graph\n",
        "\n",
        "See below for instructions on how to create this key.  When you have the key, save it to a file called `.api_key` in this directory.  If you are doing this on Google Colab, mount your local Google Drive directory and store it there."
      ],
      "metadata": {
        "id": "nh9Qci1V_L6o"
      },
      "id": "nh9Qci1V_L6o"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## If you are running this in Google Colab, install the language model using this command:"
      ],
      "metadata": {
        "id": "QZ_FeD7v_L6p"
      },
      "id": "QZ_FeD7v_L6p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-03 20:27:03.292814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (23.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.2)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "metadata": {
        "id": "QGdTsDCh_L6q",
        "outputId": "2df29dcf-39ac-4f18-fd47-3ea35d7060b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "QGdTsDCh_L6q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
        "VERBS = ['ROOT', 'advcl']\n",
        "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
        "ENTITY_LABELS = ['PERSON', 'NORP', 'GPE', 'ORG', 'FAC', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART']\n",
        "\n",
        "# api_key = open('.api_key').read()\n",
        "\n",
        "non_nc = spacy.load('en_core_web_md')\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "nlp.add_pipe('merge_noun_chunks')\n",
        "\n",
        "print(non_nc.pipe_names)\n",
        "print(nlp.pipe_names)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'merge_noun_chunks']\n"
          ]
        }
      ],
      "metadata": {
        "id": "Rziac_-L_L6r",
        "outputId": "52d376f2-08c9-4ea4-8700-09226d85bb73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Rziac_-L_L6r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Google Knowledge Graph\n",
        "\n",
        "To query the Google Knowledge Graph you will require an API key, which permits you to have 100,000 read calls per day per project for free.  That will be more than sufficient for this workshop.  To obtain your key, follow [these instructions](https://developers.google.com/knowledge-graph/how-tos/authorizing)."
      ],
      "metadata": {
        "id": "otbmJ8EO_L6r"
      },
      "id": "otbmJ8EO_L6r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def query_google(query, api_key, limit=10, indent=True, return_lists=True):\n",
        "    \n",
        "    text_ls = []\n",
        "    node_label_ls = []\n",
        "    url_ls = []\n",
        "    \n",
        "    params = {\n",
        "        'query': query,\n",
        "        'limit': limit,\n",
        "        'indent': indent,\n",
        "        'key': api_key,\n",
        "    }   \n",
        "    \n",
        "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
        "    url = service_url + '?' + urllib.parse.urlencode(params)\n",
        "    response = json.loads(urllib.request.urlopen(url).read())\n",
        "    \n",
        "    if return_lists:\n",
        "        for element in response['itemListElement']:\n",
        "\n",
        "            try:\n",
        "                node_label_ls.append(element['result']['@type'])\n",
        "            except:\n",
        "                node_label_ls.append('')\n",
        "\n",
        "            try:\n",
        "                text_ls.append(element['result']['detailedDescription']['articleBody'])\n",
        "            except:\n",
        "                text_ls.append('')\n",
        "                \n",
        "            try:\n",
        "                url_ls.append(element['result']['detailedDescription']['url'])\n",
        "            except:\n",
        "                url_ls.append('')\n",
        "                \n",
        "        return text_ls, node_label_ls, url_ls\n",
        "    \n",
        "    else:\n",
        "        return response"
      ],
      "outputs": [],
      "metadata": {
        "id": "49i7Cr45_L6s"
      },
      "id": "49i7Cr45_L6s"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions for text cleaning\n",
        "\n",
        "As with any data science project, we have a lot of cleaning to do!  These next functions do things like removing \n",
        "\n",
        "- special characters (`remove_special_characters`)\n",
        "- stop words and punctuation (`remove_stop_words_and_punct`)\n",
        "- dates (`remove_dates`)\n",
        "- duplicates (`remove_duplicates`)\n",
        "  - Duplicates crop up many times during this process and we will be battling them a lot!\n",
        "  \n",
        "Then, we can get to the heart of the matter, `create_svo_triples`."
      ],
      "metadata": {
        "id": "tJSg6Mg0_L6u"
      },
      "id": "tJSg6Mg0_L6u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def remove_special_characters(text):\n",
        "    \n",
        "    regex = re.compile(r'[\\n\\r\\t]')\n",
        "    clean_text = regex.sub(\" \", text)\n",
        "    \n",
        "    return clean_text\n",
        "\n",
        "\n",
        "def remove_stop_words_and_punct(text, print_text=False):\n",
        "    \n",
        "    result_ls = []\n",
        "    rsw_doc = non_nc(text)\n",
        "    \n",
        "    for token in rsw_doc:\n",
        "        if print_text:\n",
        "            print(token, token.is_stop)\n",
        "            print('--------------')\n",
        "        if not token.is_stop and not token.is_punct:\n",
        "            result_ls.append(str(token))\n",
        "    \n",
        "    result_str = ' '.join(result_ls)\n",
        "\n",
        "    return result_str\n",
        "\n",
        "\n",
        "def create_svo_lists(doc, print_lists):\n",
        "    \n",
        "    subject_ls = []\n",
        "    verb_ls = []\n",
        "    object_ls = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ in SUBJECTS:\n",
        "            subject_ls.append((token.lower_, token.idx))\n",
        "        elif token.dep_ in VERBS:\n",
        "            verb_ls.append((token.lemma_, token.idx))\n",
        "        elif token.dep_ in OBJECTS:\n",
        "            object_ls.append((token.lower_, token.idx))\n",
        "\n",
        "    if print_lists:\n",
        "        print('SUBJECTS: ', subject_ls)\n",
        "        print('VERBS: ', verb_ls)\n",
        "        print('OBJECTS: ', object_ls)\n",
        "    \n",
        "    return subject_ls, verb_ls, object_ls\n",
        "\n",
        "\n",
        "def remove_duplicates(tup, tup_posn):\n",
        "    \n",
        "    check_val = set()\n",
        "    result = []\n",
        "    \n",
        "    for i in tup:\n",
        "        if i[tup_posn] not in check_val:\n",
        "            result.append(i)\n",
        "            check_val.add(i[tup_posn])\n",
        "            \n",
        "    return result\n",
        "\n",
        "\n",
        "def remove_dates(tup_ls):\n",
        "    \n",
        "    clean_tup_ls = []\n",
        "    for entry in tup_ls:\n",
        "        if not entry[2].isdigit():\n",
        "            clean_tup_ls.append(entry)\n",
        "    return clean_tup_ls\n",
        "\n",
        "\n",
        "def create_svo_triples(text, print_lists=False):\n",
        "    \n",
        "    clean_text = remove_special_characters(text)\n",
        "    doc = nlp(clean_text)\n",
        "    subject_ls, verb_ls, object_ls = create_svo_lists(doc, print_lists=print_lists)\n",
        "    \n",
        "    graph_tup_ls = []\n",
        "    dedup_tup_ls = []\n",
        "    clean_tup_ls = []\n",
        "    \n",
        "    for subj in subject_ls: \n",
        "        for obj in object_ls:\n",
        "            \n",
        "            dist_ls = []\n",
        "            \n",
        "            for v in verb_ls:\n",
        "                \n",
        "                # Assemble a list of distances between each object and each verb\n",
        "                dist_ls.append(abs(obj[1] - v[1]))\n",
        "                \n",
        "            # Get the index of the verb with the smallest distance to the object \n",
        "            # and return that verb\n",
        "            index_min = min(range(len(dist_ls)), key=dist_ls.__getitem__)\n",
        "            \n",
        "            # Remve stop words from subjects and object.  Note that we do this a bit\n",
        "            # later down in the process to allow for proper sentence recognition.\n",
        "\n",
        "            no_sw_subj = remove_stop_words_and_punct(subj[0])\n",
        "            no_sw_obj = remove_stop_words_and_punct(obj[0])\n",
        "            \n",
        "            # Add entries to the graph iff neither subject nor object is blank\n",
        "            if no_sw_subj and no_sw_obj:\n",
        "                tup = (no_sw_subj, verb_ls[index_min][0], no_sw_obj)\n",
        "                graph_tup_ls.append(tup)\n",
        "        \n",
        "        #clean_tup_ls = remove_dates(graph_tup_ls)\n",
        "    \n",
        "    dedup_tup_ls = remove_duplicates(graph_tup_ls, 2)\n",
        "    clean_tup_ls = remove_dates(dedup_tup_ls)\n",
        "    \n",
        "    return clean_tup_ls"
      ],
      "outputs": [],
      "metadata": {
        "id": "USUHclmH_L6u"
      },
      "id": "USUHclmH_L6u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's get started!"
      ],
      "metadata": {
        "id": "S21TLut8_L6v"
      },
      "id": "S21TLut8_L6v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# text= wikipedia.summary('missouri')\n",
        "# get the page object for Missouri\n",
        "page = wikipedia.page(\"Missouri\")\n",
        "# get the full text of the page\n",
        "text = page.content[0:10000]\n",
        "text"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Missouri is a state in the Midwestern region of the United States. Ranking 21st in land area, it is bordered by eight states (tied for the most with Tennessee): Iowa to the north, Illinois, Kentucky and Tennessee to the east, Arkansas to the south and Oklahoma, Kansas, and Nebraska to the west. In the south are the Ozarks, a forested highland, providing timber, minerals, and recreation. The Missouri River, after which the state is named, flows through the center into the Mississippi River, which makes up the eastern border. With more than six million residents, it is the 19th-most populous state of the country. The largest urban areas are St. Louis, Kansas City, Springfield, and Columbia; the capital is Jefferson City.\\nHumans have inhabited what is now Missouri for at least 12,000 years. The Mississippian culture, which emerged at least in the ninth century, built cities and mounds before declining in the 14th century. When European explorers arrived in the 17th century, they encountered the Osage and Missouria nations. The French incorporated the territory into Louisiana, founding Ste. Genevieve in 1735 and St. Louis in 1764. After a brief period of Spanish rule, the United States acquired Missouri as part of the Louisiana Purchase in 1803. Americans from the Upland South rushed into the new Missouri Territory. Missouri was admitted as a slave state as part of the Missouri Compromise of 1820. Many from Virginia, Kentucky, and Tennessee settled in the Boonslick area of Mid-Missouri. Soon after, heavy German immigration formed the Missouri Rhineland.\\nMissouri played a central role in the westward expansion of the United States, as memorialized by the Gateway Arch. The Pony Express, Oregon Trail, Santa Fe Trail and California Trail all began in Missouri. As a border state, Missouri\\'s role in the American Civil War was complex, and it was subject to rival governments, raids, and guerilla warfare. After the war, both Greater St. Louis and the Kansas City metropolitan area became centers of industrialization and business. Today the state is divided into 114 counties and the independent city of St. Louis.\\nMissouri\\'s culture blends elements of the Midwestern and Southern United States. It is the birthplace of the musical genres ragtime, Kansas City jazz and St. Louis blues. The well-known Kansas City-style barbecue, and the lesser-known St. Louis-style barbecue, can be found across the state and beyond. Missouri is a major center of beer brewing and has some of the most permissive alcohol laws in the U.S. It is home to Anheuser-Busch, the world\\'s largest beer producer, and produces an eponymous wine produced in the Missouri Rhineland and Ozarks. Outside the state\\'s major cities, popular tourist destinations include the Lake of the Ozarks, Table Rock Lake and Branson.\\nWell-known Missourians include Chuck Berry, Sheryl Crow, Walt Disney, Edwin Hubble, Nelly, Brad Pitt, Harry S. Truman, and Mark Twain. Some of the largest companies based in the state include Cerner, Express Scripts, Monsanto, Emerson Electric, Edward Jones, H&R Block, Wells Fargo Advisors, Centene Corporation, and O\\'Reilly Auto Parts. Well-known universities in Missouri include the University of Missouri, Saint Louis University, and Washington University in St. Louis. Missouri has been called the \"Mother of the West\", the \"Cave State\", and the \"Show Me State\".\\n\\n\\n== Etymology and pronunciation ==\\nThe state is named for the Missouri River, which was named after the indigenous Missouria, a Siouan-language tribe. French colonists adapted a form of the Illinois language-name for the people: Wimihsoorita.  Their name means \"One who has dugout canoes\".The name Missouri has several different pronunciations even among its present-day inhabitants, the two most common being  (listen) mih-ZUR-ee and  (listen) mih-ZUR-ə. Further pronunciations also exist in Missouri or elsewhere in the United States, involving the realization of the medial consonant as either  or ; the vowel in the second syllable as either  or ; and the third syllable as  (phonetically [i] (listen), [ɪ] (listen), or [ɪ̈] (listen)) or . Any combination of these phonetic realizations may be observed coming from speakers of American English, except for a final [ɪ] which does not occur in most dialects (see Happy tensing). In British received pronunciation, the preferred variant is , with  being a possible alternative.The linguistic history was treated definitively by Donald M. Lance, who acknowledged that the question is sociologically complex, but no pronunciation could be declared \"correct\", nor could any be clearly defined as native or outsider, rural or urban, southern or northern, educated or otherwise. Politicians often employ multiple pronunciations, even during a single speech, to appeal to a greater number of listeners. In informal contexts respellings of the state\\'s name, such as \"Missour-ee\" or \"Missour-uh\", are occasionally used to distinguish pronunciations phonetically.\\n\\n\\n=== Nicknames ===\\nThere is no official state nickname. However, Missouri\\'s unofficial nickname is the \"Show Me State,\" which appears on its license plates. This phrase has several origins. One is popularly ascribed to a speech by Congressman Willard Vandiver in 1899, who declared that \"I come from a state that raises corn and cotton, cockleburs and Democrats, and frothy eloquence neither convinces nor satisfies me. I\\'m from Missouri, and you have got to show me.\" This is in keeping with the saying \"I\\'m from Missouri,\" which means \"I\\'m skeptical of the matter and not easily convinced.\" However, according to researchers, the phrase \"show me\" was already in use before the 1890s. Another one states that it is a reference to Missouri miners who were taken to Leadville, Colorado to replace striking workers. Since the new miners were unfamiliar with the mining methods, they required frequent instruction.Other nicknames for Missouri include \"The Lead State\", \"The Bullion State\", \"The Ozark State\", \"The Mother of the West\", \"The Iron Mountain State\", and \"Pennsylvania of the West\". It is also known as the \"Cave State\":\\u200a53\\u200a because there are more than 7,300 recorded caves in the state (second to Tennessee). Perry County is the county with the largest number of caves and the single longest cave.The official state motto is Latin: \"Salus Populi Suprema Lex Esto\", which means \"Let the welfare of the people be the supreme law.\"\\n\\n\\n== History ==\\n\\n\\n=== Early history ===\\nArchaeological excavations along river valleys have shown continuous habitation since about 9000 BCE. Beginning before 1000 CE, the people of the Mississippian culture created regional political centers at present-day St. Louis and across the Mississippi River at Cahokia, near present-day Collinsville, Illinois. Their large cities included thousands of individual residences. Still, they are known for their surviving massive earthwork mounds, built for religious, political and social reasons, in platform, ridgetop and conical shapes. Cahokia was the center of a regional trading network that reached from the Great Lakes to the Gulf of Mexico. The civilization declined by 1400 CE, and most descendants left the area long before the arrival of Europeans. St. Louis was at one time known as Mound City by the European Americans because of the numerous surviving prehistoric mounds since lost to urban development. The Mississippian culture left mounds throughout the middle Mississippi and Ohio river valleys, extending into the southeast and the upper river.\\n\\nThe land that became the state of Missouri was part of numerous different territories possessed changing and often indeterminate borders and had many different Native American and European names between the 1600s and statehood. For much of the first half of the 1700s, the west bank of the Mississippi River that would become Missouri was mostly uninhabited, something of a no man\\'s land that kept peace between the Illinois on the east bank of the Mississippi River and to the North, and the Osage and Missouri Indians of the lower Missouri Valley. In the early 1700s, French traders and missionaries explored the whole of the Mississippi Valley, named the region “Louisiana.” Around the same time, a different group of French Canadians who established five villages on the east bank of the Mississippi River placed their settlements in the le pays des Illinois, “the country of the Illinois.” When habitants – settlers of French Canadian descent – began crossing the Mississippi River to establish settlements such as Ste. Genevieve, they continued to place their settlements in the Illinois Country. At the same time, the French settlements on both sides of the Mississippi River were part of the French province of Louisiana. To distinguish the settlements in the Middle Mississippi Valley from French settlements in the lower Mississippi Valley around New Orleans, French officials and inhabitants referred to the Middle Mississippi Valley as La Haute Louisiane, “The High Louisiana,” or “Upper Louisiana.”\\nThe first European settlers were mostly ethnic French Canadians, who created their first settlement in Missouri at present-day Ste. Genevieve, about an hour south of St. Louis. They had migrated about 1750 from the Illinois Country. They came from colonial villages on the east side of the Mississippi River, where soils were becoming exhausted, and there was insufficient river bottom land for the growing population. The early Missouri settlements included many enslaved Africans and Native Americans, and slave labor was central to both commercial agriculture and the fur trade. Sainte-Geneviève became a thriving agricultural center, producing enough surplus wheat, corn and tobacco to ship tons of grain annually downriver to Lower Louisiana for trade. Grain production in the Illinois Country was critical to the survival of Lower Louisiana and especially the city of New Orleans.\\nSt. Louis was founded soon after by French fur traders, Pierre Laclède '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "metadata": {
        "id": "yqt3tjFc_L6v",
        "outputId": "35a04fc8-4ead-44ed-daa2-3b7bac73f743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "id": "yqt3tjFc_L6v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More helper functions\n",
        "\n",
        "Now that we have our text clean, we are ready to start getting information about all of the objects in the list (`get_obj_properties`) such as:\n",
        "\n",
        "- any identified node labels\n",
        "- any descriptions\n",
        "- any URLs\n",
        "\n",
        "These will be used to create node properties for all of the objects.  We end this section with allowing the ML models build into spacy to create word vector embeddings for each node based on the node description.  (If no description is provided, this will be an array of zeros.)"
      ],
      "metadata": {
        "id": "TzCvdylt_L6w"
      },
      "id": "TzCvdylt_L6w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_obj_properties(tup_ls):\n",
        "    \n",
        "    init_obj_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "\n",
        "        try:\n",
        "            text, node_label_ls, url = query_google(tup[2], api_key, limit=1)\n",
        "            new_tup = (tup[0], tup[1], tup[2], text[0], node_label_ls[0], url[0])\n",
        "        except:\n",
        "            new_tup = (tup[0], tup[1], tup[2], [], [], [])\n",
        "        \n",
        "        init_obj_tup_ls.append(new_tup)\n",
        "        \n",
        "    return init_obj_tup_ls\n",
        "\n",
        "\n",
        "def add_layer(tup_ls):\n",
        "\n",
        "    svo_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        \n",
        "        if tup[3]:\n",
        "            svo_tup = create_svo_triples(tup[3])\n",
        "            svo_tup_ls.extend(svo_tup)\n",
        "        else:\n",
        "            continue\n",
        "    \n",
        "    return get_obj_properties(svo_tup_ls)\n",
        "        \n",
        "\n",
        "def subj_equals_obj(tup_ls):\n",
        "    \n",
        "    new_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if tup[0] != tup[2]:\n",
        "            new_tup_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4], tup[5]))\n",
        "            \n",
        "    return new_tup_ls\n",
        "\n",
        "\n",
        "def check_for_string_labels(tup_ls):\n",
        "    # This is for an edge case where the object does not get fully populated\n",
        "    # resulting in the node labels being assigned to string instead of list.\n",
        "    # This may not be strictly necessary and the lines using it are commnted out\n",
        "    # below.  Run this function if you come across this case.\n",
        "    \n",
        "    clean_tup_ls = []\n",
        "    \n",
        "    for el in tup_ls:\n",
        "        if isinstance(el[2], list):\n",
        "            clean_tup_ls.append(el)\n",
        "            \n",
        "    return clean_tup_ls\n",
        "\n",
        "\n",
        "def create_word_vectors(tup_ls):\n",
        "\n",
        "    new_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if tup[3]:\n",
        "            doc = nlp(tup[3])\n",
        "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], doc.vector)\n",
        "        else:\n",
        "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], np.random.uniform(low=-1.0, high=1.0, size=(300,)))\n",
        "        new_tup_ls.append(new_tup)\n",
        "        \n",
        "    return new_tup_ls"
      ],
      "outputs": [],
      "metadata": {
        "id": "uS07cDMI_L6x"
      },
      "id": "uS07cDMI_L6x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's now run this step by step..."
      ],
      "metadata": {
        "id": "pytzm0HX_L6x"
      },
      "id": "pytzm0HX_L6x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "initial_tup_ls = create_svo_triples(text, print_lists=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7min 36s, sys: 1.25 s, total: 7min 38s\n",
            "Wall time: 7min 39s\n"
          ]
        }
      ],
      "metadata": {
        "id": "DLBeLOE9_L6x",
        "outputId": "d7412eb7-727b-467d-e852-18d7cd22e37d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DLBeLOE9_L6x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "initial_tup_ls[0:3]"
      ],
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('missouri', 'be', 'state'),\n",
              " ('missouri', 'be', 'midwestern region'),\n",
              " ('missouri', 'be', 'united states')]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "metadata": {
        "id": "ccn7QLsR_L6y",
        "outputId": "362a18ab-d54a-4396-99aa-dcfb52417cd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ccn7QLsR_L6y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "init_obj_tup_ls = get_obj_properties(initial_tup_ls)\n",
        "new_layer_ls = add_layer(init_obj_tup_ls)\n",
        "starter_edge_ls = init_obj_tup_ls + new_layer_ls\n",
        "edge_ls = subj_equals_obj(starter_edge_ls)\n",
        "#clean_edge_ls = check_for_string_labels(edge_ls)\n",
        "#clean_edge_ls[0:3]\n",
        "clean_edge_ls = edge_ls"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 670 ms, sys: 131 ms, total: 801 ms\n",
            "Wall time: 32.7 s\n"
          ]
        }
      ],
      "metadata": {
        "id": "ivD7WrYV_L6z",
        "outputId": "942f24bd-d448-48ba-e4e5-c261b8ec8b92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ivD7WrYV_L6z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "edge_ls[0:3]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('missouri', 'be', 'state', [], [], []),\n",
              " ('missouri', 'be', 'midwestern region', [], [], []),\n",
              " ('missouri', 'be', 'united states', [], [], [])]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "metadata": {
        "id": "IVu1dCvz_L6z",
        "outputId": "39e8df72-c27e-4a95-dada-6695130772ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IVu1dCvz_L6z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "edges_word_vec_ls = create_word_vectors(edge_ls)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.16 ms, sys: 995 µs, total: 4.15 ms\n",
            "Wall time: 6.71 ms\n"
          ]
        }
      ],
      "metadata": {
        "id": "bZVJxXTK_L60",
        "outputId": "28181ef1-d428-49c3-df95-0ca92320c83f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bZVJxXTK_L60"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the node and edge lists to populate the graph with the below helper functions\n",
        "\n",
        "These functions achieve the following:\n",
        "\n",
        "1. Deduping of the node list (`dedup`)\n",
        "2. Under the idea that we might want to use word embeddings, we are pulling them in as a node property.  However, Neo4j doesn't work well with numpy arrays, so we convert that array to a list of floats (`convert_vec_to_ls`).\n",
        "3. Add nodes to the graph with the `py2neo` bulk importer (`add_nodes`)\n",
        "4. Add edges (relationships) to the graph (`add_edges`)"
      ],
      "metadata": {
        "id": "7WP_zRyN_L60"
      },
      "id": "7WP_zRyN_L60"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def dedup(tup_ls):\n",
        "    \n",
        "    visited = set()\n",
        "    output_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if not tup[0] in visited:\n",
        "            visited.add(tup[0])\n",
        "            output_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4]))\n",
        "            \n",
        "    return output_ls\n",
        "\n",
        "\n",
        "def convert_vec_to_ls(tup_ls):\n",
        "    \n",
        "    vec_to_ls_tup = []\n",
        "    \n",
        "    for el in tup_ls:\n",
        "        vec_ls = [float(v) for v in el[4]]\n",
        "        tup = (el[0], el[1], el[2], el[3], vec_ls)\n",
        "        vec_to_ls_tup.append(tup)\n",
        "        \n",
        "    return vec_to_ls_tup\n",
        "\n",
        "\n",
        "def add_nodes(tup_ls):   \n",
        "\n",
        "    keys = ['name', 'description', 'node_labels', 'url', 'word_vec']\n",
        "    merge_nodes(graph.auto(), tup_ls, ('Node', 'name'), keys=keys)\n",
        "    print('Number of nodes in graph: ', graph.nodes.match('Node').count())\n",
        "    \n",
        "    return"
      ],
      "outputs": [],
      "metadata": {
        "id": "HsczmCHM_L60"
      },
      "id": "HsczmCHM_L60"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def add_edges(edge_ls):\n",
        "    \n",
        "    edge_dc = {} \n",
        "    \n",
        "    # Group tuple by verb\n",
        "    # Result: {verb1: [(sub1, v1, obj1), (sub2, v2, obj2), ...],\n",
        "    #          verb2: [(sub3, v3, obj3), (sub4, v4, obj4), ...]}\n",
        "    \n",
        "    for tup in edge_ls: \n",
        "        if tup[1] in edge_dc: \n",
        "            edge_dc[tup[1]].append((tup[0], tup[1], tup[2])) \n",
        "        else: \n",
        "            edge_dc[tup[1]] = [(tup[0], tup[1], tup[2])] \n",
        "    \n",
        "    for edge_labels, tup_ls in tqdm(edge_dc.items()):   # k=edge labels, v = list of tuples\n",
        "        \n",
        "        tx = graph.begin()\n",
        "        \n",
        "        for el in tup_ls:\n",
        "            source_node = nodes_matcher.match(name=el[0]).first()\n",
        "            target_node = nodes_matcher.match(name=el[2]).first()\n",
        "            if not source_node:\n",
        "                source_node = Node('Node', name=el[0])\n",
        "                tx.create(source_node)\n",
        "            if not target_node:\n",
        "                try:\n",
        "                    target_node = Node('Node', name=el[2], node_labels=el[4], url=el[5], word_vec=el[6])\n",
        "                    tx.create(target_node)\n",
        "                except:\n",
        "                    continue\n",
        "            try:\n",
        "                rel = Relationship(source_node, edge_labels, target_node)\n",
        "            except:\n",
        "                continue\n",
        "            tx.create(rel)\n",
        "        graph.commit(tx)\n",
        "    \n",
        "    return"
      ],
      "outputs": [],
      "metadata": {
        "id": "pRteCZUV_L61"
      },
      "id": "pRteCZUV_L61"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating some lists of tuples representing the node and edge lists\n",
        "\n",
        "For our node list, we begin with the query subject (Barack Obama), which is in `edge_ls[0][0]` and put this in the variable `orig_node_tup_ls`.  We assume a node label of `Subject` and no description or word vector.  We then add all of the objects associated with the `edges_word_vec_ls` (`obj_node_tup_ls`) and combine that with the previous variable to create `full_node_tup_ls`, which we then dedupe into `dedup_node_tup_ls`."
      ],
      "metadata": {
        "id": "GKcwicqH_L61"
      },
      "id": "GKcwicqH_L61"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "orig_node_tup_ls = [(edge_ls[0][0], '', ['Subject'], '', np.random.uniform(low=-1.0, high=1.0, size=(300,)))]\n",
        "obj_node_tup_ls = [(tup[2], tup[3], tup[4], tup[5], tup[6]) for tup in edges_word_vec_ls]\n",
        "full_node_tup_ls = orig_node_tup_ls + obj_node_tup_ls\n",
        "dedup_node_tup_ls = dedup(full_node_tup_ls)"
      ],
      "outputs": [],
      "metadata": {
        "id": "1ZoR8YSR_L62"
      },
      "id": "1ZoR8YSR_L62"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "len(full_node_tup_ls), len(dedup_node_tup_ls)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(198, 198)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "metadata": {
        "id": "D2YDuCOl_L62",
        "outputId": "8f58125d-546b-4e6b-fbdc-56a342d26707",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "D2YDuCOl_L62"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the node list that will be used to populate the graph..."
      ],
      "metadata": {
        "id": "nrbk0Cus_L62"
      },
      "id": "nrbk0Cus_L62"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "node_tup_ls = convert_vec_to_ls(dedup_node_tup_ls)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ktyysljv_L62"
      },
      "id": "ktyysljv_L62"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Populate the graph\n",
        "\n",
        "Here we establish a connection to the database, which is running on the internal Docker network called `neo4j`.  We provide it the user name and password and also create a class for matching nodes (used when we establish the edges in the graph).  Finally, we add the nodes and edges to populate the database."
      ],
      "metadata": {
        "id": "opAjK3Xv_L63"
      },
      "id": "opAjK3Xv_L63"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# If you are using a Sandbox instance, you will want to use the following (commented) line.  \n",
        "# If you are using a Docker container for your DB, use the uncommented line.\n",
        "graph = Graph(\"bolt://\", name=\"neo4j\", password=\"\")\n",
        "\n",
        "# graph = Graph(\"bolt://neo4j:7687\", name=\"neo4j\", password=\"kgDemo\")\n",
        "nodes_matcher = NodeMatcher(graph)"
      ],
      "outputs": [],
      "metadata": {
        "id": "EZ1UA1zX_L63"
      },
      "id": "EZ1UA1zX_L63"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "add_nodes(node_tup_ls)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in graph:  198\n"
          ]
        }
      ],
      "metadata": {
        "id": "abqfbVVQ_L63",
        "outputId": "a4f37934-1d38-485f-d3da-f23d0e372aa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "abqfbVVQ_L63"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "add_edges(edges_word_vec_ls)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 53/53 [00:27<00:00,  1.95it/s]\n"
          ]
        }
      ],
      "metadata": {
        "id": "aRAl-pB7_L64",
        "outputId": "b6255db6-df23-4d0e-f042-452f612e6b56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "aRAl-pB7_L64"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entity disambiguation\n",
        "\n",
        "In this notebook we will calculate the cosine similarity of the word vectors of target nodes to try and determine the likelihood of two nodes being the same entity.  Note that you could do this directly with spacy (which defaults to cosine similarity) prior to anything above through something like:\n",
        "\n",
        "```\n",
        "doc1 = nlp(text1)\n",
        "doc2 = nlp(text2)\n",
        "doc1.similarity(doc2)\n",
        "```\n",
        "\n",
        "However, since we have already calculated the word vectors for each node description above, we will just use the cosine similarity built into `scikit-learn`. \n",
        "\n",
        "Note that we are creating now a complete node list since the nodes we are comparing might be either in the Barack or the Michelle Obama node lists..."
      ],
      "metadata": {
        "id": "HpxTSRJS_L64"
      },
      "id": "HpxTSRJS_L64"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_word_vec_similarity(node1, node2, node_ls):\n",
        "    \n",
        "    node1_vec = [tup[4] for tup in node_ls if tup[0] == node1]\n",
        "    node2_vec = [tup[4] for tup in node_ls if tup[0] == node2]\n",
        "    \n",
        "    return cosine_similarity(node1_vec, node2_vec)"
      ],
      "outputs": [],
      "metadata": {
        "id": "dBJvXbgI_L65"
      },
      "id": "dBJvXbgI_L65"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "cs = get_word_vec_similarity('missouri', 'united states', dedup_node_tup_ls)\n",
        "print(cs)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.03074976]]\n"
          ]
        }
      ],
      "metadata": {
        "id": "MIRcRae9_L65",
        "outputId": "69e362cc-19bd-4d1c-a08c-e7c3ad42220e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MIRcRae9_L65"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ...OK, that one was cheating...\n",
        "\n",
        "...because the Google Knowledge Graph knew that these were the same thing.  The NER returned these as being different nodes, but Google knew better and gave the exact description for each.  This will not necessarily be the case in more complicated knowledge graphs."
      ],
      "metadata": {
        "id": "0S2CAcIn_L66"
      },
      "id": "0S2CAcIn_L66"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "cs = get_word_vec_similarity('missouri', 'missouri river', dedup_node_tup_ls)\n",
        "print(cs)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.08325318]]\n"
          ]
        }
      ],
      "metadata": {
        "id": "FwXCK9iy_L66",
        "outputId": "96381ba8-86cf-48ce-9bb2-8a3c36993c49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FwXCK9iy_L66"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem!\n",
        "\n",
        "There is a key figure missing from this graph!  (Hint: in Cypher, try the following command:\n",
        "\n",
        "```\n",
        "MATCH (n:Node)\n",
        "WHERE n.name CONTAINS 'michelle'\n",
        "RETURN n\n",
        "```\n",
        "\n",
        "In this next notebook, we are going to rectify this and build out the graph a little further."
      ],
      "metadata": {
        "id": "4l1D2vGh_L66"
      },
      "id": "4l1D2vGh_L66"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "YUCvkHvz_L66"
      },
      "id": "YUCvkHvz_L66"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}