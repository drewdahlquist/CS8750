{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drewdahlquist/CS8750/blob/main/final/01_populate_full_graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's now cut to the chase on Method 1\n",
        "\n",
        "We are going to skip ahead to the \"meat\" since the majority of the notebook before then is identical to `00`.  If you are running this on Google Colab, be sure to run the following cell:"
      ],
      "metadata": {
        "id": "kjuzxrJrSqzr"
      },
      "id": "kjuzxrJrSqzr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install py2neo\n",
        "!pip install wikipedia\n",
        "!pip install spacy #==3.0.3"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting py2neo\n",
            "  Downloading py2neo-2021.2.3-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.0/177.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from py2neo) (2022.12.7)\n",
            "Collecting interchange~=2021.0.4 (from py2neo)\n",
            "  Downloading interchange-2021.0.4-py2.py3-none-any.whl (28 kB)\n",
            "Collecting monotonic (from py2neo)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from py2neo) (23.1)\n",
            "Collecting pansi>=2020.7.3 (from py2neo)\n",
            "  Downloading pansi-2020.7.3-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pygments>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from py2neo) (2.14.0)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from py2neo) (1.16.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from py2neo) (1.26.15)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from interchange~=2021.0.4->py2neo) (2022.7.1)\n"
          ]
        }
      ],
      "metadata": {
        "id": "xHN3poiaSqzs",
        "outputId": "dc7138f6-2d2b-4cff-d34f-379410b4a50e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "xHN3poiaSqzs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import json\n",
        "import re\n",
        "import urllib\n",
        "from pprint import pprint\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from py2neo import Node, Graph, Relationship, NodeMatcher\n",
        "from py2neo.bulk import merge_nodes\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wikipedia\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.tokens import Doc, Span, Token\n",
        "\n",
        "print(spacy.__version__)"
      ],
      "outputs": [],
      "metadata": {
        "id": "vUFbaLmVSqzt"
      },
      "id": "vUFbaLmVSqzt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "We are now going to create a slightly larger graph, this time including Michelle Obama (who should obviously be there, but didn't happen to be part of Barack Obama's Wikipedia summary).  \n",
        "\n",
        "I am going to skip some of the notebook comments from the previous notebook and just include all of the functions we used then at the top of this notebook.  I am also going to create a few helper functions (the last cell of functions before the fun begins) that will make the code a bit \"easier on the eyes.\"  So please feel free to skip on ahead to the bottom of the notebook, knowing that the cell here with all of the functions is the concatenation of all of those functions from the previous notebook.\n",
        "\n",
        "## If you are running this on Google Colab, don't forget to import the language model in the following cell:"
      ],
      "metadata": {
        "id": "tsmqiepoSqzw"
      },
      "id": "tsmqiepoSqzw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "outputs": [],
      "metadata": {
        "id": "9SUEOwbzSqzx"
      },
      "id": "9SUEOwbzSqzx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
        "VERBS = ['ROOT', 'advcl']\n",
        "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
        "ENTITY_LABELS = ['PERSON', 'NORP', 'GPE', 'ORG', 'FAC', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART']\n",
        "\n",
        "api_key = open('.api_key').read()\n",
        "\n",
        "non_nc = spacy.load('en_core_web_md')\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "nlp.add_pipe('merge_noun_chunks')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function spacy.pipeline.functions.merge_noun_chunks(doc: spacy.tokens.doc.Doc) -> spacy.tokens.doc.Doc>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "metadata": {
        "id": "xUwMBum5Sqzx",
        "outputId": "301093b1-8d31-424e-d7b2-89346697535d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "xUwMBum5Sqzx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def query_google(query, api_key, limit=10, indent=True, return_lists=True):\n",
        "    \n",
        "    text_ls = []\n",
        "    node_label_ls = []\n",
        "    url_ls = []\n",
        "    \n",
        "    params = {\n",
        "        'query': query,\n",
        "        'limit': limit,\n",
        "        'indent': indent,\n",
        "        'key': api_key,\n",
        "    }   \n",
        "    \n",
        "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
        "    url = service_url + '?' + urllib.parse.urlencode(params)\n",
        "    response = json.loads(urllib.request.urlopen(url).read())\n",
        "    \n",
        "    if return_lists:\n",
        "        for element in response['itemListElement']:\n",
        "\n",
        "            try:\n",
        "                node_label_ls.append(element['result']['@type'])\n",
        "            except:\n",
        "                node_label_ls.append('')\n",
        "\n",
        "            try:\n",
        "                text_ls.append(element['result']['detailedDescription']['articleBody'])\n",
        "            except:\n",
        "                text_ls.append('')\n",
        "                \n",
        "            try:\n",
        "                url_ls.append(element['result']['detailedDescription']['url'])\n",
        "            except:\n",
        "                url_ls.append('')\n",
        "                \n",
        "        return text_ls, node_label_ls, url_ls\n",
        "    \n",
        "    else:\n",
        "        return response"
      ],
      "outputs": [],
      "metadata": {
        "id": "qJbGf-FwSqzz"
      },
      "id": "qJbGf-FwSqzz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def remove_special_characters(text):\n",
        "    \n",
        "    regex = re.compile(r'[\\n\\r\\t]')\n",
        "    clean_text = regex.sub(\" \", text)\n",
        "    \n",
        "    return clean_text\n",
        "\n",
        "\n",
        "def remove_stop_words_and_punct(text, print_text=False):\n",
        "    \n",
        "    result_ls = []\n",
        "    rsw_doc = non_nc(text)\n",
        "    \n",
        "    for token in rsw_doc:\n",
        "        if print_text:\n",
        "            print(token, token.is_stop)\n",
        "            print('--------------')\n",
        "        if not token.is_stop and not token.is_punct:\n",
        "            result_ls.append(str(token))\n",
        "    \n",
        "    result_str = ' '.join(result_ls)\n",
        "\n",
        "    return result_str\n",
        "\n",
        "\n",
        "def create_svo_lists(doc, print_lists):\n",
        "    \n",
        "    subject_ls = []\n",
        "    verb_ls = []\n",
        "    object_ls = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ in SUBJECTS:\n",
        "            subject_ls.append((token.lower_, token.idx))\n",
        "        elif token.dep_ in VERBS:\n",
        "            verb_ls.append((token.lemma_, token.idx))\n",
        "        elif token.dep_ in OBJECTS:\n",
        "            object_ls.append((token.lower_, token.idx))\n",
        "\n",
        "    if print_lists:\n",
        "        print('SUBJECTS: ', subject_ls)\n",
        "        print('VERBS: ', verb_ls)\n",
        "        print('OBJECTS: ', object_ls)\n",
        "    \n",
        "    return subject_ls, verb_ls, object_ls\n",
        "\n",
        "\n",
        "def remove_duplicates(tup, tup_posn):\n",
        "    \n",
        "    check_val = set()\n",
        "    result = []\n",
        "    \n",
        "    for i in tup:\n",
        "        if i[tup_posn] not in check_val:\n",
        "            result.append(i)\n",
        "            check_val.add(i[tup_posn])\n",
        "            \n",
        "    return result\n",
        "\n",
        "\n",
        "def remove_dates(tup_ls):\n",
        "    \n",
        "    clean_tup_ls = []\n",
        "    for entry in tup_ls:\n",
        "        if not entry[2].isdigit():\n",
        "            clean_tup_ls.append(entry)\n",
        "    return clean_tup_ls\n",
        "\n",
        "\n",
        "def create_svo_triples(text, print_lists=False):\n",
        "    \n",
        "    clean_text = remove_special_characters(text)\n",
        "    doc = nlp(clean_text)\n",
        "    subject_ls, verb_ls, object_ls = create_svo_lists(doc, print_lists=print_lists)\n",
        "    \n",
        "    graph_tup_ls = []\n",
        "    dedup_tup_ls = []\n",
        "    clean_tup_ls = []\n",
        "    \n",
        "    for subj in subject_ls: \n",
        "        for obj in object_ls:\n",
        "            \n",
        "            dist_ls = []\n",
        "            \n",
        "            for v in verb_ls:\n",
        "                \n",
        "                # Assemble a list of distances between each object and each verb\n",
        "                dist_ls.append(abs(obj[1] - v[1]))\n",
        "                \n",
        "            # Get the index of the verb with the smallest distance to the object \n",
        "            # and return that verb\n",
        "            index_min = min(range(len(dist_ls)), key=dist_ls.__getitem__)\n",
        "            \n",
        "            # Remve stop words from subjects and object.  Note that we do this a bit\n",
        "            # later down in the process to allow for proper sentence recognition.\n",
        "\n",
        "            no_sw_subj = remove_stop_words_and_punct(subj[0])\n",
        "            no_sw_obj = remove_stop_words_and_punct(obj[0])\n",
        "            \n",
        "            # Add entries to the graph iff neither subject nor object is blank\n",
        "            if no_sw_subj and no_sw_obj:\n",
        "                tup = (no_sw_subj, verb_ls[index_min][0], no_sw_obj)\n",
        "                graph_tup_ls.append(tup)\n",
        "        \n",
        "        #clean_tup_ls = remove_dates(graph_tup_ls)\n",
        "    \n",
        "    dedup_tup_ls = remove_duplicates(graph_tup_ls, 2)\n",
        "    clean_tup_ls = remove_dates(dedup_tup_ls)\n",
        "    \n",
        "    return clean_tup_ls"
      ],
      "outputs": [],
      "metadata": {
        "id": "YOL9dMkeSqz0"
      },
      "id": "YOL9dMkeSqz0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_obj_properties(tup_ls):\n",
        "    \n",
        "    init_obj_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "\n",
        "        try:\n",
        "            text, node_label_ls, url = query_google(tup[2], api_key, limit=1)\n",
        "            new_tup = (tup[0], tup[1], tup[2], text[0], node_label_ls[0], url[0])\n",
        "        except:\n",
        "            new_tup = (tup[0], tup[1], tup[2], [], [], [])\n",
        "        \n",
        "        init_obj_tup_ls.append(new_tup)\n",
        "        \n",
        "    return init_obj_tup_ls\n",
        "\n",
        "\n",
        "def add_layer(tup_ls):\n",
        "\n",
        "    svo_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        \n",
        "        if tup[3]:\n",
        "            svo_tup = create_svo_triples(tup[3])\n",
        "            svo_tup_ls.extend(svo_tup)\n",
        "        else:\n",
        "            continue\n",
        "    \n",
        "    return get_obj_properties(svo_tup_ls)\n",
        "        \n",
        "\n",
        "def subj_equals_obj(tup_ls):\n",
        "    \n",
        "    new_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if tup[0] != tup[2]:\n",
        "            new_tup_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4], tup[5]))\n",
        "            \n",
        "    return new_tup_ls\n",
        "\n",
        "\n",
        "def check_for_string_labels(tup_ls):\n",
        "    # This is for an edge case where the object does not get fully populated\n",
        "    # resulting in the node labels being assigned to string instead of list.\n",
        "    # This may not be strictly necessary and the lines using it are commnted out\n",
        "    # below.  Run this function if you come across this case.\n",
        "    \n",
        "    clean_tup_ls = []\n",
        "    \n",
        "    for el in tup_ls:\n",
        "        if isinstance(el[2], list):\n",
        "            clean_tup_ls.append(el)\n",
        "            \n",
        "    return clean_tup_ls\n",
        "\n",
        "\n",
        "def create_word_vectors(tup_ls):\n",
        "\n",
        "    new_tup_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if tup[3]:\n",
        "            doc = nlp(tup[3])\n",
        "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], doc.vector)\n",
        "        else:\n",
        "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], np.random.uniform(low=-1.0, high=1.0, size=(300,)))\n",
        "        new_tup_ls.append(new_tup)\n",
        "        \n",
        "    return new_tup_ls"
      ],
      "outputs": [],
      "metadata": {
        "id": "oztLGj4ISqz1"
      },
      "id": "oztLGj4ISqz1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def dedup(tup_ls):\n",
        "    \n",
        "    visited = set()\n",
        "    output_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if not tup[0] in visited:\n",
        "            visited.add(tup[0])\n",
        "            output_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4]))\n",
        "            \n",
        "    return output_ls\n",
        "\n",
        "\n",
        "def convert_vec_to_ls(tup_ls):\n",
        "    \n",
        "    vec_to_ls_tup = []\n",
        "    \n",
        "    for el in tup_ls:\n",
        "        vec_ls = [float(v) for v in el[4]]\n",
        "        tup = (el[0], el[1], el[2], el[3], vec_ls)\n",
        "        vec_to_ls_tup.append(tup)\n",
        "        \n",
        "    return vec_to_ls_tup\n",
        "\n",
        "\n",
        "def add_nodes(tup_ls):   \n",
        "\n",
        "    keys = ['name', 'description', 'node_labels', 'url', 'word_vec']\n",
        "    merge_nodes(graph.auto(), tup_ls, ('Node', 'name'), keys=keys)\n",
        "    print('Number of nodes in graph: ', graph.nodes.match('Node').count())\n",
        "    \n",
        "    return"
      ],
      "outputs": [],
      "metadata": {
        "id": "oXkbrBxASqz2"
      },
      "id": "oXkbrBxASqz2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def add_edges(edge_ls):\n",
        "    \n",
        "    edge_dc = {} \n",
        "    \n",
        "    # Group tuple by verb\n",
        "    # Result: {verb1: [(sub1, v1, obj1), (sub2, v2, obj2), ...],\n",
        "    #          verb2: [(sub3, v3, obj3), (sub4, v4, obj4), ...]}\n",
        "    \n",
        "    for tup in edge_ls: \n",
        "        if tup[1] in edge_dc: \n",
        "            edge_dc[tup[1]].append((tup[0], tup[1], tup[2])) \n",
        "        else: \n",
        "            edge_dc[tup[1]] = [(tup[0], tup[1], tup[2])] \n",
        "    \n",
        "    for edge_labels, tup_ls in tqdm(edge_dc.items()):   # k=edge labels, v = list of tuples\n",
        "        \n",
        "        tx = graph.begin()\n",
        "        \n",
        "        for el in tup_ls:\n",
        "            source_node = nodes_matcher.match(name=el[0]).first()\n",
        "            target_node = nodes_matcher.match(name=el[2]).first()\n",
        "            if not source_node:\n",
        "                source_node = Node('Node', name=el[0])\n",
        "                tx.create(source_node)\n",
        "            if not target_node:\n",
        "                try:\n",
        "                    target_node = Node('Node', name=el[2], node_labels=el[4], url=el[5], word_vec=el[6])\n",
        "                    tx.create(target_node)\n",
        "                except:\n",
        "                    continue\n",
        "            try:\n",
        "                rel = Relationship(source_node, edge_labels, target_node)\n",
        "            except:\n",
        "                continue\n",
        "            tx.create(rel)\n",
        "        graph.commit(tx)\n",
        "    \n",
        "    return"
      ],
      "outputs": [],
      "metadata": {
        "id": "9ziT5QSvSqz3"
      },
      "id": "9ziT5QSvSqz3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jump back in here!\n",
        "\n",
        "Now we are going to create the nodes and edges for Barack and Michelle Obama and populate the database with them.\n",
        "\n",
        "To do so, I have combined several of the steps in the previous notebook into a couple of helper functions."
      ],
      "metadata": {
        "id": "k57a5eLKSqz4"
      },
      "id": "k57a5eLKSqz4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def edge_tuple_creation(text):\n",
        "    \n",
        "    initial_tup_ls = create_svo_triples(text)\n",
        "    init_obj_tup_ls = get_obj_properties(initial_tup_ls)\n",
        "    new_layer_ls = add_layer(init_obj_tup_ls)\n",
        "    starter_edge_ls = init_obj_tup_ls + new_layer_ls\n",
        "    edge_ls = subj_equals_obj(starter_edge_ls)\n",
        "    edges_word_vec_ls = create_word_vectors(edge_ls)\n",
        "    \n",
        "    return edges_word_vec_ls\n",
        "\n",
        "\n",
        "def node_tuple_creation(edges_word_vec_ls):\n",
        "    \n",
        "    orig_node_tup_ls = [(edges_word_vec_ls[0][0], '', ['Subject'], '', np.random.uniform(low=-1.0, high=1.0, size=(300,)))]\n",
        "    obj_node_tup_ls = [(tup[2], tup[3], tup[4], tup[5], tup[6]) for tup in edges_word_vec_ls]\n",
        "    full_node_tup_ls = orig_node_tup_ls + obj_node_tup_ls\n",
        "    cleaned_node_tup_ls = check_for_string_labels(full_node_tup_ls)\n",
        "    #dedup_node_tup_ls = dedup(cleaned_node_tup_ls)\n",
        "    dedup_node_tup_ls = cleaned_node_tup_ls\n",
        "    node_tup_ls = convert_vec_to_ls(dedup_node_tup_ls)\n",
        "    \n",
        "    return node_tup_ls    "
      ],
      "outputs": [],
      "metadata": {
        "id": "mmvVYzueSqz4"
      },
      "id": "mmvVYzueSqz4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# If you are using a Sandbox instance, you will want to use the following (commented) line.  \n",
        "# If you are using a Docker container for your DB, use the uncommented line.\n",
        "graph = Graph(\"bolt://\", name=\"neo4j\", password=\"\")\n",
        "\n",
        "# graph = Graph(\"bolt://neo4j:7687\", name=\"neo4j\", password=\"kgDemo\")\n",
        "nodes_matcher = NodeMatcher(graph)"
      ],
      "outputs": [],
      "metadata": {
        "id": "RS9RrxpUSqz5"
      },
      "id": "RS9RrxpUSqz5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%time\n",
        "# barack_text = wikipedia.summary('missouri')\n",
        "# barack_edges_word_vec_ls = edge_tuple_creation(barack_text)\n",
        "# barack_node_tup_ls = node_tuple_creation(barack_edges_word_vec_ls)\n",
        "mo_page = wikipedia.page(\"Missouri\")\n",
        "mo_text = mo_page.content[0:10000] \n",
        "mo_edges_word_vec_ls = edge_tuple_creation(mo_text)\n",
        "mo_node_tup_ls = node_tuple_creation(mo_edges_word_vec_ls)\n",
        "\n",
        "# michelle_text = wikipedia.summary('michelle obama')\n",
        "# michelle_edges_word_vec_ls = edge_tuple_creation(michelle_text)\n",
        "# michelle_node_tup_ls = node_tuple_creation(michelle_edges_word_vec_ls)\n",
        "ks_page = wikipedia.page(\"Kansas\")\n",
        "ks_text = ks_page.content[0:10000] \n",
        "ks_edges_word_vec_ls = edge_tuple_creation(ks_text)\n",
        "ks_node_tup_ls = node_tuple_creation(ks_edges_word_vec_ls)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 18min 1s, sys: 3.68 s, total: 18min 5s\n",
            "Wall time: 18min 19s\n"
          ]
        }
      ],
      "metadata": {
        "id": "1Fm5uAcwSqz5",
        "outputId": "2bb40c90-89de-42c3-8367-31a8ffd4f674",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1Fm5uAcwSqz5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "full_node_ls = mo_node_tup_ls + ks_node_tup_ls\n",
        "full_edge_ls = mo_edges_word_vec_ls + ks_edges_word_vec_ls\n",
        "full_dedup_node_tup_ls = dedup(full_node_ls)\n",
        "print(len(full_node_ls), len(full_dedup_node_tup_ls))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "436 416\n"
          ]
        }
      ],
      "metadata": {
        "id": "2FJJF1t7Sqz6",
        "outputId": "2628f85c-bcf3-46d2-d3a0-164383afe427",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2FJJF1t7Sqz6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "add_nodes(full_dedup_node_tup_ls)\n",
        "add_edges(full_edge_ls)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in graph:  416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 91/91 [00:56<00:00,  1.61it/s]\n"
          ]
        }
      ],
      "metadata": {
        "id": "sgn9EwGCSqz6",
        "outputId": "f2b295c0-5506-4259-8118-be98039ed15c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "sgn9EwGCSqz6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next steps...\n",
        "\n",
        "We now have a graph that we can tinker with.  I would encourage you to explore the Cypher queries that can be found in `cypher_queries/`.  You will need to run several of these in order to use the next notebook, where we will explore doing some machine learning on the graph.  Note that any Cypher query can be done using `py2neo` or the official Neo4j python driver.  In the workshop, I will demonstrate these using the Neo4j browser, which you will find at `localhost:7474`."
      ],
      "metadata": {
        "id": "T65GsVQZSqz7"
      },
      "id": "T65GsVQZSqz7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "94LcAy6sSqz7"
      },
      "id": "94LcAy6sSqz7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}